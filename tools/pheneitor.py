#!/usr/bin/env python3

# Pheneitor
# Created: 2025-07-02
# Author: Eugenio Guzm√°n <eugenio.guzman@tec.mx>

import argparse
import io
import signal
from typing import Iterable
from asteval import Interpreter
import pandas as pd
import numpy as np
import functools
import re
import sys
import os
import hashlib
from pathlib import Path

# The order that Pheneitor expects columns in the guide table
# (it expects a one-line header although it ignores it)
GTABLE_COLUMN_ORDER = [
    "active",
    "prelude",
    "checknas",
    "inclusion",
    "exclusion",
    "type",
    "cases",
    "controls",
    "overlap",
    "transform",
    "covars"
]

# Values to consider for the active column
ACCEPTED_TRUES = [True, 1, "True", "TRUE", "true", "T", "Yes", "YES", "yes", "Y"]


HASH_CHUNK_SIZE = 8192

def parse_args():
    parser = argparse.ArgumentParser(
        prog="pheneitor",
        description="Generates phenotypes given a table"
    )
    parser.add_argument(
        "--data", type=str, action="append", help="Tab separated data tables", required=True
    )
    parser.add_argument(
        "--traits", type=str, action="store", help="Tab separated table that dictates how phenotypes are calculated", required=True
    )
    parser.add_argument(
        "--fam", type=str, action="store", help="Path to PLINK fam file (required for correct ordering of samples)"
    )
    parser.add_argument(
        "--map", type=str, action="append", help="Tab separated mapping of IDs in data tables (col 1) to IDs in PLINK fam (col 2) with no header"
    )
    parser.add_argument(
        "--subset", type=str, action="store", help="For the samples not listed in this file, sets all phenotypes to NA"
    )
    parser.add_argument(
        "--pheno", type=str, nargs='+', help="Only generate these phenotypes"
    )
    parser.add_argument(
        "--cache", action="store_true", help="Whether to only calculate new phenotypes or not according to the summary table (experimental)"
    )
    parser.add_argument(
        "--use-links", action="store_true", help="Whether to use symlinks for identical GCTA covar/pheno files (experimental)"
    )
    parser.add_argument(
        "--snptest-out", type=str, action="store", help="Destination of SNPTest-formatted phenotype/covariate table generated by this program (optional)"
    )
    parser.add_argument(
        "--gcta-out-prefix", type=str, action="store", help="Filename prefix of GCTA phenotype + covariate tables generated by this program (optional)"
    )
    parser.add_argument(
        "--summary-out", type=str, action="store", help="Destination of summary table generated by this program (defaults to stdout)"
    )
    parser.add_argument(
        "--gcta-summary-files", action="store_true", help="Saves per-phenotype summary files given by --gcta-out prefix (experimental)"
    )

    args = parser.parse_args()

    kwargs = {}
    
    if args.gcta_out_prefix:
        if not args.fam:
            parser.error("--fam is required when --gcta-pefix is given")
        kwargs["gcta_dest"] = args.gcta_out_prefix

    if args.snptest_out:
        if not args.fam:
            parser.error("--fam is required when --snptest-out is given")
        kwargs["phenotable_output"] = args.snptest_out
    if args.fam:
        kwargs["fam_path"] = args.fam
    if args.subset:
        kwargs["subsetsamples_path"] = args.subset
    if args.summary_out:
        kwargs["phenosummary_output"] = args.summary_out
    if args.pheno:
        kwargs["subset"] = args.pheno
    if args.map:
        kwargs["mappers"] = args.map
    if args.cache:
        kwargs["cache"] = True
    if args.use_links:
        kwargs["use_links"] = True
    if args.gcta_summary_files:
        kwargs["indiv_summaries"] = True

    
    main(args.data, args.traits, **kwargs)

def file_hash(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        for chunk in iter(lambda: f.read(HASH_CHUNK_SIZE), b""):
            h.update(chunk)
    return h.digest()

def gcta_tsv_and_hash(df: pd.DataFrame):
    buffer = io.StringIO()
    df.to_csv(buffer, sep="\t", float_format="{:.6g}".format, header=False)
    data = buffer.getvalue()
    return (data, hashlib.sha256(data.encode()).hexdigest())



def main(
        table_paths, guidetable_path,
        fam_path=None, subsetsamples_path=None, phenotable_output=None, phenosummary_output=sys.stdout, gcta_dest=None,
        subset=None, mappers=None, cache=False, use_links=False, indiv_summaries=False):
    # This is to allow piping to commands that prematurely close the output stream
    signal.signal(signal.SIGPIPE, signal.SIG_DFL)

    if fam_path is not None:
        fam_data = pd.read_csv(fam_path, sep=r"\s", usecols=[0,1], names=["famid", "sample"], engine='python')
        famids = fam_data.set_index("sample")["famid"]
        fam_samples = fam_data["sample"].values
    else:
        fam_samples = None
    ss_samples = fam_samples  # sample subset

    if subsetsamples_path is not None:
        # If a FAM file is given, subset the samples to those in the list in subsetsamples_path
        # otherwise just keep the samples in subsetsamples_path

        sss = set(pd.read_csv(subsetsamples_path, sep="\t", names=["sample"])["sample"].values)
        ss_samples = np.array(list(
            (set(ss_samples) & sss) if ss_samples is None else sss
        ))

    sample_id_mapping = {}
    if mappers is not None:
        for m in mappers:
            sample_id_mapping |= pd.read_csv(m, sep="\t", index_col=0, usecols=[0, 1], names=[0, 1])[1].to_dict()

    do_cache = cache and phenosummary_output != sys.stdout and file_exists(phenosummary_output)
    detected_phenos = None
    files = {}
    if do_cache:
        detected_phenos = []
        if gcta_dest is not None:
            gp = Path(gcta_dest)
            iext = ".phen"
            exts = [".phen", ".covar", ".qcovar"]
            gcta_phenos = []
            if (gcta_dest.endswith("/") or  gcta_dest.endswith(".")) and gp.is_dir():
                for file in gp.iterdir():
                    fname = file.name
                    if fname.endswith(iext):
                        gcta_phenos.append(fname[:-len(iext)])
                    if use_links:
                        for ext in exts:
                            if fname.endswith(ext):
                                files[file_hash(file)] = str(file)
            else:
                gpn = gp.name + "." if not (gcta_dest.endswith("/") or gcta_dest.endswith(".")) else ""
                for file in gp.parent.iterdir():
                    fname = file.name
                    if fname.startswith(gpn) and fname.endswith(iext):
                        gcta_phenos.append(fname[len(gpn):-len(iext)])
                    if use_links:
                        for ext in exts:
                            if fname.endswith(ext):
                                files[file_hash(file)] = str(file)
            detected_phenos.append(set(gcta_phenos))

        if phenotable_output is not None and file_exists(phenotable_output):
            cols = list(pd.read_csv(phenotable_output, nrows=0, sep=" ", index_col=0).columns)
            detected_phenos.append(set(cols))

        if detected_phenos is not None:
            detected_phenos = list(functools.reduce(lambda a, x: x.intersection(a), detected_phenos))

    try:
        errors, counts, df, data, gtable, aliases, debug_messages = make_phenotable(
            table_paths, guidetable_path, subset=subset,
            samples=ss_samples, mapping=sample_id_mapping,
            summary_for_cache=phenosummary_output if do_cache else None,
            detected_phenos=detected_phenos if do_cache else None
        )
    except NoPhenotypesException:
        print("No phenotypes to be generated", file=sys.stderr)
        sys.exit(10)

    # TODO: think of a better strategy to idenitfy errors
    haserr = errors.apply(lambda x: x.str.contains("^ERROR:", regex=True, flags=re.MULTILINE)).any(axis=1)

    # My Python installation's type system is a little clumsy,
    # I have to manually specify that summary_frames is an iterable...
    # it should work even without the type annotations
    summary_frames: Iterable[pd.DataFrame] = [gtable, haserr.rename("has_error"), counts.add_prefix("counts:"), errors.add_prefix("error:"), debug_messages.rename("debug_messages")]
    summary_df = pd.concat(summary_frames, axis=1)
    if do_cache:
        orig_sdf = pd.read_csv(phenosummary_output, sep="\t", index_col=0)
        orig_sdf =  orig_sdf[~orig_sdf.index.isin(summary_df.index)]
        summary_df = pd.concat([orig_sdf, summary_df])

    try:
        summary_df \
            .map(lambda x: x.replace("\n", "\\n").replace("\t", "\\t") if isinstance(x, str) else x) \
            .rename_axis(index="name") \
            .to_csv(phenosummary_output, sep="\t", quoting=3)
    except BrokenPipeError:
        pass

    # All covars must be in the --input data frame, otherwise the program exits with code 1
    # (aliases are allowed, see make_phenotable)
    inccovars = pd.Index(gtable.loc[~haserr, "covars"].str.split(r"\s*,\s*").explode().unique()).dropna().to_series().isin(set(data.columns) | set(aliases.keys()))

    if not inccovars.all():
        print(
            ("WARN" if phenotable_output is None else "ERROR") +
            f": the following covars were not found in the input table(s): {', '.join(inccovars[~inccovars].index)}",
            file=sys.stderr
        )
        if phenotable_output is not None:
            print(f"Refusing to output phenotype table due to missing covars", file=sys.stderr)
        sys.exit(1)
    
    covars = data[inccovars.index.map(lambda x: aliases.get(x, x))]
    covars.columns = covars.columns.map(lambda x: x[2:] if x.startswith("!#") else x)
    covartypes = []

    # SNPTest requires a second header row indicating column type
    # (D -> discrete covar, C -> continuous covar, B -> binary pheno, P -> continuous pheno)
    for c in covars.columns:
        if pd.api.types.is_bool_dtype(covars[c]):
            covartypes.append("D")
        elif pd.api.types.is_any_real_numeric_dtype(covars[c]):
            covartypes.append("C")
        else:
            covartypes.append("D")
    covars.columns = pd.MultiIndex.from_arrays([covars.columns, covartypes])
    out_df = covars

    if phenotable_output is None and gcta_dest is None:
        return

    phenos: Iterable[pd.DataFrame] = [
        df.columns.to_series().rename("column"),
        df.columns.to_series().map(lambda x: x.split("$")[0]).rename("pheno")
    ]
    phenos = pd.concat(phenos, axis=1)
    
    df = df.drop(columns=phenos.loc[phenos["pheno"].isin(haserr.index[haserr]), "column"], errors="ignore")
    phenos = phenos.loc[df.columns]

    df.columns = pd.MultiIndex.from_arrays([df.columns, phenos.merge(gtable["type"], left_on="pheno", right_index=True, how="left")["type"].map({
        "case-control": "B",
        "regression": "P",
        "transform": "!"
    })])
    out_df = out_df.join(df)
    out_df.index.name = ("sample", "0")

    for col, coltype in out_df.columns:
        if (coltype == "B") | pd.api.types.is_bool_dtype(out_df[(col, coltype)].dtype):
            # Make binary values appear as 1s and 0s
            out_df[col] = out_df[col].astype(pd.Int16Dtype())
        elif (coltype == "P") | (coltype == "C"):
            out_df[col] = out_df[col].astype(pd.Float32Dtype())
    
    if fam_samples is not None:
        # VERY IMPORTANT: the output file must follow the same ordering as the FAM file
        # otherwise all results produced by SNPTest will be wrong!!
        out_df = out_df.reindex(fam_samples)
    
    if phenotable_output is not None:
        try:
            if do_cache:
                save_df = pd.read_csv(phenotable_output, sep=" ", index_col=0, header=[0, 1], na_values=["NA"], quoting=3)
                #save_df = save_df.drop(columns=[("sample", "0")]).set_index(save_df[("sample", "0")])
                save_df = out_df.drop(columns=save_df.columns.intersection(out_df.columns)).join(save_df)
            else:
                save_df = out_df
            save_df = save_df.reset_index()
            if (save_df.columns.get_level_values(1) == "!").sum() > 0:
                print("WARNING: excluding some phenotypes that have the 'transform' type which is not supported in SNPTest output", file=sys.stderr)
            save_df = save_df.loc[:, save_df.columns.get_level_values(1) != "!"]
            save_df.to_csv(phenotable_output, float_format="{:.6g}".format, sep=" ", na_rep="NA", index=False, quoting=3)
        # Avoid errors due to piping into commands that do not read the output entirely (i.e., head)
        except BrokenPipeError:
            pass
    
    out_df = out_df.reset_index()
    # Save GCTA files
    if gcta_dest is not None:
        out_df = out_df.drop(columns=[("sample", "0")]).set_index(out_df[("sample", "0")])
        out_df.index = pd.MultiIndex.from_arrays([out_df.index.map(famids), out_df.index], names=["famid", "sample"])
        dfdt = out_df.columns.get_level_values(1)
        out_df = out_df.droplevel(1, axis=1)
        for pheno in summary_df.index:
            pheno_cols = phenos.loc[phenos["pheno"] == pheno, "column"]
            dest = lambda x: gcta_dest + ("." if not gcta_dest.endswith("/") or gcta_dest.endswith(".") else "") + pheno + "." + x
            if indiv_summaries:
                print("SAVING TO " + dest("summary"), file=sys.stderr)
                summary_df.loc[pheno].map(lambda x: x.replace("\n", "\\n").replace("\t", "\\t") if isinstance(x, str) else x).to_csv(dest("summary"), sep="\t", header=False)
            
            if haserr.loc[pheno]:
                continue
            #dest = lambda x: gcta_dest + ("." if not Path(gcta_dest).is_dir() else "") + pheno + "." + x
            phen = out_df[pheno_cols.values].fillna(-9)
            
            fd_phen, fv_phen = gcta_tsv_and_hash(phen)
            dest_phen = dest("phen")
            
            if use_links and fv_phen in files:
                if file_exists(dest_phen):
                    os.remove(dest_phen)
                os.symlink(Path(files[fv_phen]).name, dest_phen)
            else:
                if use_links:
                    files[fv_phen] = dest_phen
                try:
                    #phen.to_csv(dest_phen, sep="\t", float_format="{:.6g}".format, header=False)
                    with open(dest_phen, "w") as f:
                        f.write(fd_phen)
                except BrokenPipeError:
                    pass
                
            if hasvalue(gtable.loc[pheno, "covars"]):
                pheno_covars = [(x[2:] if x.startswith("!#") else aliases.get(x, x)) for x in [y.strip() for y in gtable.loc[pheno, "covars"].split(",")]]
            else:
                pheno_covars = []

            # Only discrete covars are saved to the .covar file
            covar = out_df.loc[:, (dfdt == "D") & (out_df.columns.isin(pheno_covars))].astype(pd.StringDtype()).fillna("-9") # TODO: is -9 a valid NA here?

            if covar.shape[1] > 0:
                fd_covar, fv_covar = gcta_tsv_and_hash(covar)
                dest_covar = dest("covar")
                if use_links and fv_covar in files:
                    if file_exists(dest_covar):
                        os.remove(dest_covar)
                    os.symlink(Path(files[fv_covar]).name, dest_covar)
                else:
                    if use_links:
                        files[fv_covar] = dest_covar
                    try:
                        #covar.to_csv(dest_covar, sep="\t", float_format="{:.6g}".format, header=False)
                        with open(dest_covar, "w") as f:
                            f.write(fd_covar)
                    except BrokenPipeError:
                        pass

            # Only quantitative covars are saved to the .qcovar file
            qcovar = out_df.loc[:, (dfdt == "C") & (out_df.columns.isin(pheno_covars))].fillna(-9) # TODO: is -9 a valid NA here?
            if qcovar.shape[1] > 0:
                fd_qcovar, fv_qcovar = gcta_tsv_and_hash(qcovar)
                dest_qcovar = dest("qcovar")
                if use_links and fv_qcovar in files:
                    if file_exists(dest_qcovar):
                        os.remove(dest_qcovar)
                    os.symlink(Path(files[fv_qcovar]).name, dest_qcovar)
                else:
                    if use_links:
                        files[fv_qcovar] = dest_qcovar
                    try:
                        #qcovar.to_csv(dest_qcovar, sep="\t", float_format="{:.6g}".format, header=False)
                        with open(dest_qcovar, "w") as f:
                            f.write(fd_qcovar)
                    except BrokenPipeError:
                        pass

def file_exists(path):
    # TODO: Document
    f = Path(path)
    if f.is_file():
        return True
    return False

def nawarn(ser, mask=None, action=None, num=None, critname=None):
    """
    Generate a warning regarding NA values in a Series.

    Parameters:
        ser (Series): The series to analyze
        mask (Series): A boolean series containing False where values should be ignored.
        action (str): Message indicating what the action will be for those NAs.
        critname (str): The section that produced the NA values.

    Returns:
        str | None: Warning message if NAs detected.
    """
    if mask is None:
        nnas = ser.isna().sum()
    else:
        nnas = ser[mask].isna().sum()
    if nnas > 0:
        return "WARN: " + ("" if num is None else num2ordinal(num) + " ") + \
            (f"{critname} " if critname is not None  else "")+ \
            f"expression produced NAs (n={nnas:,})" + \
            (f"; will {action}" if action is not None else "")
    else:
        return None
    
def addmsg(mailbox, recipient, msg):
    """
    Add an ERROR/WARN message to a "mailbox" dictionary (messages separated by semicolon).

    Parameters:
        mailbox (dict): Where to put the message.
        recipient (any): The key of the dictionary where to put the message.
        msg (str): The message to append.
    """
    if recipient not in mailbox or mailbox[recipient] is None:
        mailbox[recipient] = msg
        return
    if msg is None:
        return
    mailbox[recipient] = mailbox[recipient] + "; " + msg

num2ordinal = lambda i: {0: "1st", 1: "2nd", 2: "3rd"}.get(i, f"{i+1}th")

def hasvalue(x):
    """
    Simple check for detecting the presence of values.

    Parameters:
        x (any): Value to check for nullity.
    
    Returns:
        bool: True if not empty.
    """
    return (not pd.isna(x)) and (x != "")

def evalcrit(expr, evalctx, index, allow_multiple=False, dtype="any"):
    """
    Evaluate a criterion of the guide table allowing for Series objects.

    Parameters:
        expr (str): Expression to evaluate as Python code.
        evalctx (Interpreter): Interpreter that evaluates the code.
        index (Index): The order in which rows will be sorted.
        allow_multiple (bool): Whether or not a tuple of Series will be accepted.
        dtype ("boolean" | "numeric" | "any"): The dtype of the Series returned by expr.

    Returns:
        tuple: A tuple containing the following elements:
            - error (str | None): Detected error messages.
            - ser (Series | tuple | None): If expr is valid, a Series (or tuple of Series) of the expected dtype.
    """
    err = None
    sers = evalctx(expr)
    if evalctx.error:
        err = "ERROR: " + evalctx.error[0].msg
        sers = None
    else:
        dtype_test = {
            "boolean": pd.api.types.is_bool_dtype,
            "numeric": pd.api.types.is_any_real_numeric_dtype,
            "any": lambda *_: True
        }[dtype]

        sers = (sers,) if type(sers) != tuple else sers
        wrongs = []

        for i in range(len(sers)):
            if not isinstance(sers[i], pd.Series):
                wrongs.append(f"{num2ordinal(i)} item (a {str(type(sers[i]))})")
            elif not dtype_test(sers[i]):
                wrongs.append(f"{num2ordinal(i)} item (a series of {str(type(sers[i].dtype))})")
        
        if len(wrongs) > 0:
            err = f"WARNING: ignoring the following items: {', '.join(wrongs)}"
        
        orig_serlen = len(sers)
        sers = [s.reindex(index) for s in sers if isinstance(s, pd.Series)]

        if orig_serlen > 1 and (not allow_multiple):
            err = f"ERROR: expected a single series, got {len(sers)}"
            sers = None
        elif len(sers) < 1 and orig_serlen > 0:
            err = f"ERROR: no valid series found, see warnings above"
            sers = None
        elif orig_serlen < 1:
            err = f"ERROR: no series provided"
            sers = None
        
        if sers is not None:
            for s in sers:
                s = s.reindex(index)

    return (err, sers[0] if (not allow_multiple) and (type(sers) == list) else sers)
    

def set_or_append_count(target, index, count):
    """
    Append a number to a possibly missing key in a dictionary as a string using commas for delimiters.

    Parameters:
        target (dict): Dictionary where counts are stored.
        index (any): Key in the dictionary specifying where to store the value.
        count (int): Number to add to the list.
    """
    if index in target and target[index] is not None:
        target[index] = str(target[index]) + ", " + str(count)
    else:
        target[index] = count

def report_counts(target, name, stepname, ccdata, pass_filter):
    """
    Report counts of cases, controls, and non-null values at the given step.

    Parameters:
        target (dict): Dictionary where counts are stored.
        name (str): Name of the phenotype.
        stepname (str): Name of the step.
        ccdata (dict | None): Dictionary containing boolean Series of "cases" and "controls" if applicable.
        pass_filter (Series): Series indicating rows that pass the filter at the given step.
    """
    if ccdata is not None:
        #set_or_append_count(target, (name, f"values_{stepname}"), (pass_filter & (ccdata["cases"] | ccdata["controls"])).sum())
        if "cases" in ccdata and ccdata["cases"] is not None:
            set_or_append_count(target, (name, f"cases:{stepname}"), int((pass_filter & ccdata["cases"]).sum()))
        if "controls" in ccdata and ccdata["controls"] is not None:
            set_or_append_count(target, (name, f"controls:{stepname}"), int((pass_filter & ccdata["controls"]).sum()))
    #else:
    #    set_or_append_count(target, (name, f"values_{stepname}"), pass_filter.sum())
    set_or_append_count(target, (name, f"values:{stepname}"), int(pass_filter.sum()))

class NoPhenotypesException(Exception):
    pass

def make_phenotable(table_paths, guidetable_path, samples=None, subset=None, mapping={}, summary_for_cache=None, detected_phenos=None):
    """
    Generate a phenotype table from a guide table and list of data tables.

    Parameters:
        table_paths (list): Paths to tables whose columns are used in the guide table.
        guidetable_path (str): Path to a table dictating how phenotypes are computed.
        samples (array | None): An optional subset of samples for which to calcualte the phenotypes.
        subset (list | None): If given, only these phenotypes specified will be calculated.

    Returns:
        tuple: A tuple containing the following elements:
            - errors (DataFrame): Error messages detected at each step (columns) for every phenotype (rows).
            - counts (DataFrame): Counts of cases/controls/non-NA values at every step (columns) for every phenotye (rows).
            - phenotypes (DataFrame): Values for every phenotype (columns) and every individual (rows).
            - df (DataFrame): Original phenotype input data read from table_paths.
            - gtable (DataFrame): Guide table read from guidetable_path.
            - aliases (dict): Field name aliases.
    """
    df = pd.DataFrame()

    for table_path in table_paths:

        if table_path.lower().endswith(".parquet"):
            table = pd.read_parquet(table_path)
        else:
            # Tables must be in TSV format with a single header row. Its first column must be the
            # index column with sample names matching those in the FAM file (or if not provided, 
            # they must at least be consistent).
            table = pd.read_csv(table_path, sep="\t", index_col=0, dtype_backend="pyarrow")

        # Map sample names according to mappers
        table.index = table.index.map(lambda x: mapping.get(x, x))

        # This avoids a "KeyError: DataType(null)" with df.convert_dtypes() later on
        table = table.loc[:, ~table.isna().all()]

        if not samples is None and not bool(set(table.index) & set(samples)):
            print(f"WARN: no (useful) samples found in {table_path}", file=sys.stderr)

        # If a field is declared more than once in different tables, the field declared in the
        # leftmost table of table_paths takes precedence
        df = df.join(table.drop(columns=df.columns.intersection(table.columns)), how="outer")
    
    df = df.convert_dtypes()  # pyarrow dtypes allow loading nullable data although many methods
                              # are exclusive to pandas native dtypes

    # Aliases are defined as the last period-separated part of the column name.
    # They are only generated for the following cases:
    # - Columns that have unique aliases
    # - Columns whose table (first part of the period-separated column name)
    #   is "PARTICIPANTE" or "PARTICIPANT"
    # Aliases are convinient ways (syntactic sugar) to specify the fully-
    # quallified field name as TABLE.OTHER.PARTS (which would become PARTS)

    #cnames = df.columns.to_series().map(lambda x: x.split(".")[-1])
    
    cnames = []
    for c in df.columns:
        parts = c.split(".")
        for i in range(len(parts)):
            alias = ".".join(parts[i:])
            force = (alias == c) or (parts[0] == "PARTICIPANTE") or (parts[0] == "PARTICIPANTS")
            cnames.append({"column": c, "alias": alias, "force": force})
    cnames = pd.DataFrame(cnames)

    aliases = {}
    for a, rows in cnames.groupby("alias"):
        if rows.shape[0] == 1 or rows["force"].sum() == 1:
            aliases[a] = rows.sort_values("force", ascending=False)["column"].values[0]

    # Remove samples outside the subset if specified
    if samples is not None:
        df = df.reindex(samples)
    
    symtab = symtable(df, aliases)

    cols = []  # List of generated phenotype Series
    errors = {}  # Errors found for every phenotype at each step
    debug_msgs = {} # Debug messages for every phenotype
    ns_at_step = {}  # Counts of non-NA values, and cases + controls (if applicable)
                     #    for each phenotype at each step
    namelist = set()  # Set of phenotype names already defined

    # Guide table - must be tab separated and have the samples in the first column
    # The other columns should be ordered according to GTABLE_COLUMN_ORDER
    # A single header line must be present, although it is not parsed
    gp = guidetable_path.lower()
    if not (gp.endswith(".yaml") or gp.endswith(".yml")):
        gtable = pd.read_csv(guidetable_path, sep="\t", index_col=0).rename_axis(index="name")
        gtable.columns = GTABLE_COLUMN_ORDER + gtable.columns[len(GTABLE_COLUMN_ORDER):].to_list()
    else:
        # Feature (2025-11-12): YAML files are supported. Each key is a phenotype. Valid field names
        # are listed in GTABLE_COLUMN_ORDER. Remember to set "active: True".
        import yaml
        with open(guidetable_path) as f:
            gtable = yaml.safe_load(f)
            gtable = {
                k1: {
                    # lists are transformed into comma delimited strings
                    k2: ", ".join(v2) if type(v2) == list else v2
                    for k2, v2 in v1.items()
                } for k1, v1 in gtable.items()
            }
        gtable = pd.DataFrame(gtable).T

    if subset is not None:
        gtable = gtable.loc[subset]

    if summary_for_cache is not None:
        sfc = pd.read_csv(summary_for_cache, sep="\t", index_col=0)
        sfc = sfc.map(lambda x: x.replace("\\n", "\n").replace("\\t", "\t")  if isinstance(x, str) else x)
        sfc = sfc.reindex(index=gtable.index, columns=gtable.columns)
        comparison = ((sfc.isna() & gtable.isna()) | (sfc == gtable))

        skip_phenos = sfc.index[comparison.all(axis=1).values & sfc.index.isin(detected_phenos)]
        gtable = gtable[~gtable.index.isin(skip_phenos)]

    # For every phenotype definition
    for name, entry in gtable.iterrows():
        # Only process if it is active
        if not entry["active"] in ACCEPTED_TRUES:
            continue

        if name in namelist:
            addmsg(errors, (name, "general"), "WARN: repeated phenotype name")
        namelist.add(name)
        
        # The prelude is executed before anything else and variables are kept
        dbgctx = {}
        symtab["debug"] = debug_function(dbgctx)
        aeval = Interpreter(symtable=symtab)

        if "prelude" in entry and hasvalue(entry["prelude"]):
            aeval(entry["prelude"])
            if aeval.error:
                addmsg(errors, (name, "general"), "ERROR: (prelude) " + aeval.error[0].msg)

        # Pre-calculate cases and controls for keeping track of counts
        ccdata = None
        if entry["type"] == "case-control":
            ccdata = {}
            for critname in "cases", "controls":
                ccdata[critname] = None
                err, ser = evalcrit(entry[critname], aeval, df.index, dtype="boolean")
                addmsg(errors, (name, critname), err)
                ccdata[critname] = ser

        # Initially, no samples are filtered
        filtered = pd.Series(np.ones(df.shape[0], dtype=bool), index=df.index)
        
        report_counts(ns_at_step, name, "prefilter", ccdata, filtered)

        # Checknas removes rows that have a missing value at ANY of these columns
        if hasvalue(entry["checknas"]):
            err, colstocheck = evalcrit(entry["checknas"], aeval, df.index, allow_multiple=True, dtype="any")
            addmsg(errors, (name, "checknas"), err)
            if colstocheck is not None:
                for col in colstocheck:
                    filtered &= ~col.isna()

        report_counts(ns_at_step, name, "after_checknas", ccdata, filtered)

        for critname, invert in [
            ("inclusion", False),
            ("exclusion", True)
        ]:
            if critname in entry and hasvalue(entry[critname]):
                err, sers = evalcrit(entry[critname], aeval, df.index, allow_multiple=True, dtype="boolean")
                addmsg(errors, (name, critname), err)
                if sers is not None:
                    for i, ser in enumerate(sers):
                        #
                        # Hidden feature (do not document): Multiple inclusion/exclusion filters may be
                        #                                   specified by separating them with commas (i.e.,
                        #                                   returning a tuple instead of a series); however,
                        #                                   the behavior may be unexpected... inclusion
                        #                                   criteria are concatenated by the AND() function
                        #                                   while exclusion criteria are concatenated by the
                        #                                   OR() function. In other words, a row must meet
                        #                                   all inclusion criteria and no exclusion criterion.
                        #
                        #        What to recommend instead: Tell the user to explicitly use the AND() or OR()
                        #                                   functions.
                        #
                        addmsg(errors, (name, critname), nawarn(ser, filtered, action=f"{'include' if invert else 'exclude'} those samples", num=i if len(sers) > 1 else None, critname=critname))
                        filtered &= (~ser if invert else ser).fillna(invert)
                        report_counts(ns_at_step, name, "after_" + critname, ccdata, filtered)
        
        if entry["type"] == "case-control":
            filtered2 = filtered.copy()  # Warn the user only once for NAs in cases + controls
            for critname in "cases", "controls":
                ser = ccdata[critname]
                if ser is not None:
                    addmsg(errors, (name, critname), nawarn(ser, filtered2, critname=critname))
                    filtered2 &= ~ser.isna()  # filter NAs found out from warnings loop
                else:
                    addmsg(errors, (name, critname), f"WARN: {critname} expression is either empty or not valid")
            
            cc = pd.Series(index=df.index, dtype=pd.BooleanDtype())
            if ccdata["cases"] is not None:
                cc[ccdata["cases"].fillna(False) & filtered] = True
            if ccdata["controls"] is not None:
                cc[ccdata["controls"].fillna(False) & filtered] = False
            if ccdata["cases"] is not None and ccdata["controls"] is not None:
                intersection = (ccdata["cases"] & ccdata["controls"] & filtered)
                intersectionsz = intersection.sum()  # number of samples that overlap
                overlaps_handled = False
                if "overlap" in entry and hasvalue(entry["overlap"]):
                    overlaps_handled = True
                    omethod = entry["overlap"].lower()
                    if omethod == "cases":
                        cc.loc[intersection] = True
                    elif omethod == "controls":
                        cc.loc[intersection] = False
                    elif omethod == "missing":
                        cc = cc.mask(intersection)
                    else:
                        overlaps_handled = False
                        addmsg(errors, (name, "overlap"), f"WARN: unrecognized value '{entry['overlap']}' for overlap hangling, use either 'cases', 'controls', or 'missing'")

                if intersectionsz > 0 and not overlaps_handled:
                    addmsg(errors, (name, "overlap"), f"WARN: cases overlap with controls in {intersectionsz:,} individuals; treating them as NAs; consider setting an overlap handling method (either 'cases', 'controls', or 'missing')")
                    cc = cc.mask(intersection)
            ns_at_step[(name, "values")] = (~cc.isna()).sum()
            ns_at_step[(name, "cases")] = (cc == True).sum()
            ns_at_step[(name, "controls")] = (cc == False).sum()
            cols.append(cc.rename(name))
        elif entry["type"] == "regression":
            err, ser = evalcrit(entry["transform"], aeval, df.index, dtype="numeric")
            addmsg(errors, (name, "transform"), err)
            if ser is not None:
                if pd.api.types.is_bool_dtype(ser):
                    addmsg(errors, (name, "transform"), "WARN: transformation yields a series of boolean dtype")
                addmsg(errors, (name, "transform"), nawarn(ser, filtered, critname="transform"))

                nonfinite = ((~np.isfinite(ser)) & (~pd.isna(ser))).sum()
                if nonfinite > 0:
                    addmsg(errors, (name, "transform"), f"WARN: found {nonfinite} non-finite values; treating them as NA")
                
                ser = ser.mask((~filtered) | ~np.isfinite(ser))
                ns_at_step[(name, "values")] = (~ser.isna()).sum()
                cols.append(ser.rename(name))
        elif entry["type"] == "transform":
            err, sers = evalcrit(entry["transform"], aeval, df.index, allow_multiple=True)
            addmsg(errors, (name, "transform"), err)
            if sers is not None:
                addmsg(errors, (name, "transform"), nawarn(ser, filtered, critname="transform"))
                ser_nas = []
                for i, ser in enumerate(sers):
                    if isinstance(ser, pd.Series):
                        ser_nas.append((~ser.isna()).sum())
                        cols.append(ser.rename(name + f"${i}"))
                ns_at_step[(name, "values")] = ",".join(f"{s:.0f}" for s in ser_nas)
        else:
            addmsg(errors, (name, "general"), f"ERROR: unknown type '{entry['type']}'; expected either 'case-control' or 'regression'")
        
        if "covars" in entry and hasvalue(entry["covars"]):
            modify_covars = False
            covars = entry["covars"].split(",")
            for i in range(len(covars)):
                covar = covars[i].strip()
                if covar in df.columns:
                    continue
                elif covar in aliases.keys():
                    continue
                elif covar in aeval.symtable:
                    new_covar = "!#" + covar
                    new_covar_val = aeval.symtable[covar]
                    if new_covar in df.columns and (df[new_covar] != new_covar_val).fillna(False).any():
                        print(f"WARNING: covariate {covar} is re-declared with a different value in phenotype {name}", file=sys.stderr)
                    df[new_covar] = new_covar_val
                    covars[i] = new_covar
                    modify_covars = True
            if modify_covars:
                gtable.loc[name, "covars"] = ",".join(covars)
        
        debug_msgs[name] = " ".join(f"[{k}] {v}" for k, v in dbgctx.items())


    values_prod = sum(([stratum + ":" + moment for moment in ["prefilter", "after_checknas", "after_inclusion", "after_exclusion"]] for stratum in ["values", "cases", "controls"]), [])
    errcols = ["general", "checknas", "inclusion", "exclusion", "cases", "controls", "overlap", "transform"]
    nscols = [*values_prod, "values","cases","controls"]
    if len(cols) == 0:
        raise NoPhenotypesException()
    
    return (
        (pd.Series(errors).unstack().reindex(columns=errcols) if len(errors) > 0 else pd.DataFrame(columns=errcols)).astype(pd.StringDtype()),
        (pd.Series(ns_at_step, dtype=pd.StringDtype()).unstack() if len(errors) > 0 else pd.DataFrame(columns=ns_at_step)).reindex(columns=nscols),
        pd.concat(cols, axis=1),
        df,
        gtable,
        aliases,
        pd.Series(debug_msgs)
    )
    

# This class is merely a dummy class for setting variables with
# nested attributes
class TableSection:
    pass

def debug_function(context):
    def debug(data, label=None):
        message = ""
        if isinstance(data, pd.Series):
            if pd.api.types.is_bool_dtype(data.dtype):
                message = f"T:{(data).sum():.0f}; F:{(~data).sum():.0f}; NA:{data.isna().sum():.0f}"
            elif pd.api.types.is_numeric_dtype(data.dtype):
                message = f"N:{(~data.isna()).sum():.0f}; M:{(data).mean():.3g}; S:{(data).std():.3g}; Q0:{(data).min():.3g}; Q1:{(data).quantile(.25):.3g}; Q2:{(data).median():.3g}; Q3:{(data).quantile(.75):.3g}; Q4:{(data).max():.3g}"
            else:
                message = f"N:{(~data.isna()).sum():.0f}"
        else:
            message = str(data)
        
        if label is None:
            label = "msg"
            tag = "msg_1"
        else:
            tag = str(label)
        num = 1
        while tag in context:
            num += 1
            tag = label + f"_{num}"
        context[tag] = message
    return debug


def mapper_get_notes(table, field):
    # TODO: Document
    pattern = re.compile(r"\b" + re.escape(field) + r"\b")
    def mapper_instance_get_notes(x):
        if pd.isna(x):
            return pd.NA
        tbnotes = x.split("[")
        for tbn in tbnotes:
            if tbn.startswith(table + "]"):
                notes = "]".join(tbn.split("]")[1:]).split(";")
                notes = [note for note in notes if pattern.search(note)]
                return notes
        return pd.NA
    return mapper_instance_get_notes

def mapper_get_old_val(column):
    # TODO: Document
    cfs = column.split(".")
    table = cfs[0]
    field = ".".join(cfs[1:])
    pattern = re.compile("^" + re.escape(field) + r":([^=]*)=>.*$")
    get_notes_inst = mapper_get_notes(table, field)
    def mapper_instance_get_old_val(x):
        if pd.isna(x):
            return pd.NA
        notes = get_notes_inst(x)
        if notes is pd.NA:
            return pd.NA
        notes = [note for note in notes if pattern.search(note)]
        if len(notes) == 0:
            return pd.NA
        note = notes[0]
        match = pattern.match(note)
        return match[1]
    return mapper_instance_get_old_val

def symtable(df, aliases):
    """
    Generate a dictionary with variables accessible to the sandboxed interpreter.

    Parameters:
        df (DataFrame): Data table from which columns are extracted into variables.
        aliases (dict): Dictionary mapping column names to shorter identifiers.

    Returns:
        dict: Dict mapping identifiers to table columns and custom functions
    """

    # ========================
    # --- Custom functions ---
    # ========================

    def bins(x, undeflow_val, cutoffs):
        """
        Divide a series into bins by specifying cutoffs and the values they take.

        Parameters:
            x (Series): Data to be divided into bins.
            underflow_val (any): The value of the lowest bin below any cutoff.
            cutoffs (dict): Mapping between cutoffs and the values the data takes at that point onwards.

        Returns:
            Series: Data binned into segments specified by the cutoffs dictionary.

        """
        return functools.reduce(
            lambda a, z: a.where(x < z[0], z[1]), sorted(cutoffs.items()), pd.Series(np.full(x.shape[0], undeflow_val), index=x.index)
        ).mask(x.isna()).convert_dtypes()
    
    def interp(x, underflow_val, overflow_val, steps):
        """
        Interpolate the values of a series in a list of points.

        Parameters:
            x (Series): Data to be interpolated.
            underflow_val (any): The value the data takes when it lies below the interpolation range.
            overflow_val (any): The value the data takes when it lies above the interpolation range.
            steps (dict): Mapping between input-scale coordinates and output-scale coordinates.

        Returns:
            Series: Interpolated data.
        """
        return pd.Series(
            np.where(
                (x < min(steps)).fillna(False),
                underflow_val,
                np.where(
                    (x > max(steps)).fillna(False),
                    overflow_val,
                    np.interp(x.fillna(0), *zip(*sorted(steps.items())))
                )
            ),
            index=x.index
        ).mask(x.isna()).convert_dtypes()
    
    def ifelse(cond, x, y):
        """
        Use different values in a Series depending on a condition.

        Parameters:
            cond (Series): Boolean series dictating which value is taken (x or y).
            x (Series): Values produced by the function when cond evaluates to True.
            y (Series): Values produced by the function when cond evaluates to False.

        Returns:
            Series: Series with x wherever cond is True and y wherever cond is False.
        """
        return pd.Series(np.where(
            cond.astype(pd.BooleanDtype()).fillna(False).astype(bool),
            x.reindex(cond.index) if isinstance(x, pd.Series) else x,
            y.reindex(cond.index) if isinstance(y, pd.Series) else y,
        ), index=cond.index).mask(cond.isna()).convert_dtypes()
    
    def clamp(x, low, high):
        """
        Restrict values to a given range.

        Parameters:
            x (any): Data/datum to restrict.
            low (any): The lowest value the function will produce.
            high (any): The highest value the function will produce.

        Returns:
            any: Restricted value(s).
        """
        return np.minimum(high, np.maximum(x, low))

    def AND(*x):
        """
        Computes the logical AND between arguments.

        Parameters:
            *x (any): Elements to consider.

        Returns:
            any: Logical AND of the given elements.
        """
        return functools.reduce(lambda a, z: a & z , x[1:], x[0])
    
    def OR(*x):
        """
        Computes the logical OR between arguments.

        Parameters:
            *x (any): Elements to consider.

        Returns:
            any: Logical OR of the given elements.
        """
        return functools.reduce(lambda a, z: a | z , x[1:], x[0])
    
    def NOT(x):
        """
        Computes the logical NOT of the argument.

        Parameters:
            x (any): Elements to negate.

        Returns:
            any: Logical NOT of the given element.
        """
        return ~x
    
    def between(x, low, high):
        """
        Checks if values fall within the given range (inclusive).

        Parameters:
            x (any): Data/datum to check if in range.
            low (any): Lower cutoff.
            high (any): Upper cutoff.

        Returns:
            any: Boolean representation of the input data evaluated to True wherever x is inside the range.
        """
        return (x >= low) & (x <= high)
    
    def isin(x, arr):
        """
        Checks if the argument is any of the values in a list. Equivalent to x.isin(arr).

        Parameters:
            x (Series): Data to check for its presence in the list.
            arr (any): The list containing values of interest.

        Returns:
            Series: Boolean Series with True wherever the corresponding element of x is found in arr.
        """
        return x.isin(arr)

    def fillna(x, y):
        """
        Replaces all NAs in a Series with a value. Equivalent to x.fillna(y).

        Parameters:
            x (Series): Data containing NAs.
            y (any): Value to replace NAs with.

        Returns:
            Series: Series similar to x, but with NAs replaced with y.
        """
        return x.fillna(y)
    
    def cast(x, y):
        """
        Converts the type of a Series to the given type. Equivalent to x.astype(y).

        Parameters:
            x (Series): Data to convert.
            y (any): The data type (dtype).

        Returns:
            Series: The Series x casted to dtype y.
        """
        return y.astype(x)
    
    def randomcol():
        """
        Generates a Series with uniform random numbers.

        Returns:
            Series: Random numbers between 0 and 1 with the same index as the rest of the data.
        """
        return pd.Series(np.random.uniform(size=df.shape[0]), index=df.index)
    
    def rnd():
        """
        Generates a single uniform random number.

        Returns:
            float: Number between 0 and 1.
        """
        
        return np.random.uniform()
    
    def zscore(data, strata=None):
        """
        Computes the z-score of the given data by optionally stratifying.

        Parameters:
            data (Series): Data to compute the z-score.
            strata (None|Series): Optional Series whose values represent the strata.

        Returns:
            Series: The calculated z-scores.
        """
        if strata is not None:
            mu = data.groupby(strata).mean()
            sigma = data.groupby(strata).std()
            return (data - strata.map(mu)) / strata.map(sigma)
        mu = data.mean()
        sigma = data.std()
        return (data - mu)/sigma
    
    def norm(data, strata=None):
        """
        Computes the min-max normalized values of the given data by optionally stratifying.

        Parameters:
            data (Series): Data to compute the normalization.
            strata (None|Series): Optional Series whose values represent the strata.

        Returns:
            Series: The rescaled values.
        """
        if strata is not None:
            hi = data.groupby(strata).max()
            lo = data.groupby(strata).min()
            rn = hi - lo
            return (data - strata.map(lo)) / (strata.map(rn))
        hi = data.max()
        lo = data.min()
        rn = hi - lo
        return (data - lo) / rn
    
    def quantile(data, strata=None):
        """
        Computes the quantiles corresponding to the values of the data by optionally stratifying.

        Parameters:
            data (Series): Data for determining the quantiles.
            strata (None|Series): Optional Series whose values represent the strata.

        Returns:
            Series: The quantiles of the original data.
        """
        if strata is not None:
            out = pd.Series(name=data.name, index=data.index)
            for _, stratum in data.groupby(strata):
                out.loc[stratum.index] = stratum.rank(pct=True)
            return out
        return data.rank(pct=True)
    
    def get_notes(note_column, column):
        # TODO: Document
        cfs = column.split(".")
        table = cfs[0]
        field = ".".join(cfs[1:])
        return note_column.map(mapper_get_notes(table, field)).astype(pd.StringDtype())

    def get_old_value(note_column, column):
        # TODO: Document
        return note_column.map(mapper_get_old_val(column)).astype(pd.StringDtype())


    # ========================
    # --- Column variables ---
    # ========================
    
    sections = {}

    colaliases = {
        a: df[c] for a, c in aliases.items() if "." not in a
    }

    for a, c in aliases.items():
        if "." not in a:
            continue
        parts = a.split(".")
        if parts[0] not in sections:
            sections[parts[0]] = TableSection()
        
        curr = sections[parts[0]]
        for p in parts[1:-1]:
            setattr(curr, p, TableSection())
            curr = getattr(curr, p)
        setattr(curr, parts[-1], df[c])

    # =========================
    # --- Exported symbable ---
    # =========================
    
    return sections | colaliases | {
        "pi": np.pi,
        "e": np.e,
        "log": np.log,
        "exp": np.exp,
        "sqrt": np.sqrt,
        "cbrt": np.cbrt,
        "pow": np.power,
        "floor": np.floor,
        "ceil": np.ceil,
        "round": np.round,
        "array": np.array,
        "isna": pd.isna,
        "isreal": np.isfinite,
        "max": np.maximum,
        "min": np.minimum,
        "zscore": zscore,
        "norm": norm,
        "quantile": quantile,
        "rnd": rnd,
        "randomcol": randomcol,
        "bins": bins,
        "interp": interp,
        "ifelse": ifelse,
        "clamp": clamp,
        "between": between,
        "isin": isin,
        "fillna": fillna,
        "get_notes": get_notes,
        "get_old_value": get_old_value,
        "AND": AND,
        "OR": OR,
        "NOT": NOT,
        "TRUE": True,   "true": True,   "T": True,
        "FALSE": False, "false": False, "F": False,
        "NA": pd.NA,
        "NaN": np.nan,
        "cast": cast,
        "int": int,
        "float": float,
        "bool": bool,
        "str": str,
        "Int": pd.Int32Dtype(),
        "Float": pd.Float32Dtype(),
        "Bool": pd.BooleanDtype(),
        "Str": pd.StringDtype(),
        "list": list,
        "series": pd.Series
    }

if __name__ == "__main__":
    parse_args()